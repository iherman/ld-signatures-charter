<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <title>Linked Data Signatures Working Group Charter—Explainer and Use Cases</title>
  <script src="https://www.w3.org/Tools/respec/respec-w3c" class="remove" defer></script>
  <script class="remove">
// All config options at https://respec.org/docs/
var respecConfig = {
  specStatus: "base",
  editors: [{
    name: "Ivan Herman",
    url: "https://www.w3.org/People/Ivan/",
    company: "W3C",
    w3cid: 7382,
    orcid: "0000-0003-0782-2704",
    companyURL: "https://www.w3.org",
  }, {
    name: "Manu Sporny",
    url: "http://manu.sporny.org/",
    company: "Digital Bazaar",
    companyURL: "https://digitalbazaar.com/",
    w3cid: 41758
  }, {
    name: "Aidan Hogan",
    url: "http://aidanhogan.com/",
    company: "DCC, Universidad de Chile",
    companyURL: "https://www.dcc.uchile.cl/",
    orcid: "0000-0001-9482-1982"
  }],
//    name: "Alan Karp",
//    url: "https://alanhkarp.com/",
//    company: "Earth Computing",
//    companyURL: "https://www.earthcomputing.io/",
//  }
  github: "w3c/lds-wg-charter",
  localBiblio: {
    "aidan-2017" : {
      title: "Canonical Forms for Isomorphic and Equivalent RDF Graphs: Algorithms for Leaning and Labelling Blank Nodes",
      authors: [
        "Aidan Hogan"
      ],
      href : "http://aidanhogan.com/docs/rdf-canonicalisation.pdf",
      publisher: "ACM Transactions on the Web",
      date : 2017
    },
    "arnold-longley-2020": {
      title: "RDF Dataset Normalization",
      authors : [
        "Rachel Arnold",
        "Dave Longley"
      ],
      href: "https://lists.w3.org/Archives/Public/public-credentials/2021Mar/att-0220/RDFDatasetCanonicalization-2020-10-09.pdf",
      publisher: "W3C",
      date : "2020-10-09"
    }
  }
};
  </script>
  <style>
.issue {
  background: teal !important;
  color: #FFC !important ;
}

.issue::before {
  content: "\2009";
}

.issue::after {
  content: "\2009";
}

.todo {
  color: #900 !important;
  background-color: #FFC !important;
}

.rdf {
  font-style: italic;
  font-family: cursive;
}
  </style>
</head>

<body>
  <section id="abstract">
    <p>
      This is a supporting document for the proposed <a href="index.html">Linked Data Signatures Working Group Charter</a>, providing some extra explanation of the problem space and associated use cases and requirements.
    </p>
  </section>

  <section id='canonicalize'>
    <h2>Terminology</h2>

    <p>
      For a precise definition of the various terms and concepts, the reader should refer to the formal RDF specification [[rdf11-concepts]].
    </p>

    <dl>
      <dt id='dataset'>RDF Datasets</dt>
      <dd>
        <p>
          <span class='rdf'>R</span>, <span class='rdf'>R'</span> and <span class='rdf'>S</span> each denote an <a href="https://www.w3.org/TR/rdf11-concepts/#section-dataset">RDF Dataset</a> [[rdf11-concepts]].
        </p>
      </dd>

      <dt id='identical'>Identical RDF Datasets</dt>
      <dd>
        <p>
          <span class='rdf'>R = S</span> denotes that <span class='rdf'>R</span> and <span class='rdf'>S</span> are <em>identical</em> RDF Datasets. 
        </p>
        <p>
          Two RDF Datasets are identical if and only if they have the same <a href="https://www.w3.org/TR/rdf11-concepts/#section-dataset">default graph</a> (under set equality) and the same set of <a href="https://www.w3.org/TR/rdf11-concepts/#section-dataset">named graphs</a> (under set equality). </p>
         <p>
          If <span class='rdf'>R</span> and <span class='rdf'>S</span> are identical, we may equivalently say that they are the <em>same</em> RDF Dataset. 
        </p>
      </dd>

      <dt id='isomorphic'>Isomorphic RDF Datasets</dt>
      <dd>
        <p>
          <span class='rdf'>R ≈ S</span> denotes that <span class='rdf'>R</span> and <span class='rdf'>S</span> are <a href='https://www.w3.org/TR/rdf11-concepts/#section-dataset-isomorphism'><em>isomorphic</em></a> RDF Datasets. 
        </p>
        <p>
          Two isomorphic RDF Datasets are identical except for differences in how their individual blank nodes are labeled. In particular, <span class='rdf'>R</span> is isomorphic with <span class='rdf'>S</span> if and only if it is possible to map the blank nodes of <span class='rdf'>R</span> to the blank nodes of <span class='rdf'>S</span> in a one-to-one manner, generating an RDF dataset <span class='rdf'>R'</span> such that <span class='rdf'>R' = S</span>.
        </p>
      </dd>
      
      <dt id='canonicalization'>RDF Dataset Canonicalization</dt>
      <dd>
        <p>
          RDF Dataset Canonicalization is a function <span class='rdf'>C</span> that maps an RDF Dataset to an RDF Dataset in a manner that satisfies the following two properties for all RDF Datasets <span class='rdf'>R</span> and <span class='rdf'>S</span>:
        </p>
        <ul>
          <li> <span class='rdf'>R ≈ C(R)</span>; and</li> 
          <li> <span class='rdf'>C(R) = C(S)</span> if and only if <span class='rdf'>R ≈ S</span>.</li>
        </ul>
        <p id="canonical_form">
          We may refer to <span class='rdf'>C(R)</span> as the <em>canonical form</em> of <span class='rdf'>R</span> (under <span class='rdf'>C</span>&nbsp;).
        </p>
        <p>
          Such a canonicalization function can be implemented, in practice, as a procedure that deterministically re-labels all blank nodes of an RDF Dataset in a one-to-one manner, without depending on the particular set of blank node labels used in the input RDF Dataset.
        </p>
      </dd>
    </dl>

    <div class=note id=abstract_concept>
      <p>
        It is important to emphasize that the term “canonicalization” is used here in its very generic form, described as:
      </p>

      <blockquote>
        In computer science, <b>canonicalization</b> […] is a process for converting data that has more than one possible representation into a “standard”, “normal”, or canonical form. <br> Source: <a href="https://en.wikipedia.org/wiki/Canonicalization">Wikipedia</a>.
      </blockquote>

      <p>
        Canonicalization, as used in the context of this document and the proposed charter, is indeed defined on <em>an abstract data model</em> (i.e., on <a href="https://www.w3.org/TR/rdf11-concepts/#section-dataset">RDF Dataset</a> [[rdf11-concepts]]), regardless of a specific serialization. (It could also be referred to as a “canonical relabelling scheme”).
        It is therefore very different from the usage of the term in, for example, the “Canonical XML” [[xml-c14n11]] or the “JSON Canonicalization Scheme” [[rfc8785]] specifications. 
        Any comparison with those documents can be misleading.
      </p>


    </div>

    <section id="generalProblem">
      <h2>The General Problem Space</h2>
      <p>
        Though canonical labeling procedures for directed and undirected graphs have been studied for several decades, only in the past 10 years have two generalized approaches been proposed for RDF Graphs and RDF Datasets:
      </p>

      <ol>
        <li>
          The algorithms defined by Aidan Hogan in [[aidan-2017]], reviewed through the anonymous scholarly peer review process, implemented by the author.
        </li>
        <li>
          The algorithm defined by Rachel Arnold and Dave Longley, see [[arnold-longley-2020]], <a href="https://lists.w3.org/Archives/Public/public-credentials/2021Apr/att-0032/Mirabolic_Graph_Iso_Report_2020_10_19.pdf">reviewed</a> by <a href="https://lists.w3.org/Archives/Public/public-credentials/2021Apr/0032.html">experts at Mirabolic Consulting</a>, implemented and deployed via, e.g., the <a href="https://github.com/digitalbazaar/jsonld-signatures">JSON-LD Signatures package</a> used in several JSON-LD Signature suites.
        </li>
      </ol>

      <p>
        The introduction of <a href="http://aidanhogan.com/docs/rdf-canonicalisation.pdf">Aidan Hogan’s paper</a>&nbsp;[[aidan-2017]] also contains a more thorough description of the underlying mathematical challenges.
      </p>
    </section>
    
    <section id="noProblem"><h2>Where there is no problem</h2>
      <p>When a digitally signed file is transferred from one system to another and used <em>as is</em>, there is no need for any processing other than checking the digital signature. This is as true for RDF as any other data format and existing signature and other cryptographic proof methods may be used. However, RDF has many serializations, notably <a href="https://www.w3.org/TR/rdf-syntax-grammar/">RDF/XML</a>, <a href="https://www.w3.org/TR/turtle/">Turtle</a>, <a href="https://www.w3.org/TR/json-ld/">JSON-LD</a>, <a href="https://www.w3.org/TR/n-quads/">N-quads</a> and, informally, <a href="https://digitalbazaar.github.io/cbor-ld-spec/">CBOR-LD</a>. The same RDF dataset can be transferred in any of those serializations and is typically processed by querying the underlying RDF graph, not by querying the original serialized data. So, yes, you <em>can</em> process JSON-LD as JSON (that's a key design feature) but you can equally extract the triples and work with that, or transform it into CBOR-LD for use in a constrained environment like a QR code etc. It's this range of serializations and usage scenarios that creates the need to be able to sign the underlying dataset, irrespective of its serialization.</p>
    </section>
  </section>

  <section id="goals">
    <h2>Defining an RDF Dataset Hash and/or Signing RDF Datasets</h2>

    <p>
      Signing an RDF Dataset follows, roughly, the same approach as for XML&nbsp;[[xmldsig-core1]]. For an RDF Dataset <span class='rdf'>R</span> this implies two steps:
    </p>

    <ol>
      <li>
        use an <a href="#canonicalization">RDF Dataset Canonicalization</a> function <span class='rdf'>C</span> to calculate <span class='rdf'>C(R)</span>;
      </li>
      <li>
        serialize <span class='rdf'>C(R)</span> to quads&nbsp;[[n-quads]] and sort the resulting set of quads;
      </li>
      <li id=hash>
        apply a (traditional) hashing function <span class='rdf'>h</span> on the result of the serialization to yield <span class='rdf'>h(R)</span> (the cryptographic hash of the Dataset).
      </li>
      <li id=sign>
        apply a digital signature function on <span class='rdf'>h(R)</span>.
      </li>
    </ol>

    <p>
      The main challenge for the Working Group is to provide a standard for the <a href="#canonicalization">RDF Dataset Canonicalization function</a>. That paves the way for uniquely identifying RDF Datasets, as well as methods to digitally sign RDF Datasets.
    </p>
  </section>

  <section id=usage>
    <h2>Use Cases and Requirements</h2>

    <p>
      Some typical use cases for RDF Dataset Canonicalization and/or signatures are:
    </p>

    <dl>
      <dt id="detectingChanges">Detecting changes in Datasets</dt>
      <dd>
        When processing RDF Datasets over a period of time, determining if information has changed is helpful. For example, knowing if information has changed helps with data cache invalidation, detecting if expected data has been tampered with or modified, or when debugging unexpected changes in source RDF Datasets.
        <br>Requirement: An RDF Dataset Canonicalization algorithm.
      </dd>
      <dt id="spaceEfficient">Space-efficient verification of the contents of Datasets</dt>
      <dd>
        If unique identification of RDF Datasets is possible, one can cryptographically hash the information to establish a storage-efficient way to verify that the information has not changed over time. One property of cryptographic digests is that one can verify data integrity. For example, a small device sending an RDF Dataset to a remote storage location can compute a cryptographic digest for later use in verifying that all the data arrived intact and has not been tampered with. <br>Requirement: An RDF Dataset Canonicalization algorithm whose output can be used as input a cryptographic digest function.
        <br>(Contributed by <a href="https://alanhkarp.com/">Alan Karp.)</a>
      </dd>
      <dt id="secretConfirmation">Secret confirmation of the contents of Datasets</dt>
      <dd>
        Since a cryptographic digest is a one-way function, and serves as an abbreviation for the entire RDF Dataset, one can use it in places where secrecy is desired. For example, when ensuring that the transaction history on a distributed ledger is the same between two services, two systems could keep track of the list of transactions in their respective ledgers. Canonicalizing and cryptographically hashing the list of transactions should result in the same cryptographic hash without either party needing to share the list of transactions with the other. <br>Requirement: An RDF Dataset Canonicalization algorithm whose cryptographic digest can be shared to establish the unique identity of the RDF Dataset.
        <br>(Contributed by <a href="https://alanhkarp.com/">Alan Karp.)</a>
      </dd>
      <dt id="dSig">
        Annotating Datasets with digital signatures and other digital proofs
      </dt>
      <dd>
        When publishing or transmitting an RDF Dataset, clearly articulating the entity that asserted the data and protecting it from undetected modification is useful for mission critical systems. For example, understanding the issuer of a Verifiable Credential and ensuring that it is evident when a Verifiable Presentation has been tampered with underlies the trustworthiness of the encoded information. <br>Requirement: A way of encoding and verifying a digital signature on an RDF Dataset.
      </dd>
      <dt id="anchoringOnDLT">
        Anchoring the existence of Datasets to a Distributed Ledger
      </dt>
      <dd>
        New forms of digital proofs, such as proof of work, proof of stake, proof of existence, and proof of elapsed time, have demonstrated that there are useful forms of cryptographic digital proofs that go beyond digital signatures. For example, anchoring an RDF Dataset that expresses a land deed to a Distributed Ledger (aka blockchain) can establish a proof of existence in a way that does not depend on a single point of failure, such as a local government office.
        <br>Requirement: A way of encoding and verifying a digital proof that is not a digital signature on an RDF Dataset.
      </dd>
      <dt id="skolem">
        Generating canonical Skolem IRIs for blank nodes
      </dt>
      <dd>
        <a href="https://www.w3.org/TR/rdf11-concepts/#section-skolemization">Skolem IRIs</a> have been proposed in RDF 1.1 as a way to replace blank nodes with IRIs in application scenarios where it is preferable to avoid the use of blank nodes. Rather than using an ad hoc scheme to generate Skolem IRIs to replace blank nodes, an alternative is to generate Skolem IRIs in a deterministic manner, such that compliant implementations will generate the same IRIs to replace the same blank nodes in isomorphic copies of an RDF graph or dataset. Such a procedure will produce a canonical version of a Skolemized RDF graph that can then be used in the context of several of the use-cases mentioned previously.
        <br>Requirement: An RDF Dataset Canonicalization algorithm that produces canonical labels for blank nodes and a convention to compute Skolem IRIs from these labels.
      </dd>
      <dt id="qr">Constrained data transfer</dt>
      <dd>The <a href="#spaceEfficient">space-efficient</a> and <a href="#dSig">digital proofs</a> use cases above both hint at scenarios where data transfer is severely constrained. Such transfer might be achieved using an optical data carrier, such as a QR code or Data Matrix, or a radio frequency data carrier such as an NFC tag.
        <br>
        Requirement: Lossless conversion between highly compact and less compact forms of informationally identical RDF Datasets.</dd>
      <dt id="semanticConsistency">Semantic consistency of multi-part datasets</dt>
      <dd>The <a href="#detectingChanges">detecting changes</a> and <a href="#spaceEfficient">space-efficient verification</a> use-cases above can be leveraged in situations where a graph or dataset semantically relies on one or several other graph(s), which it refers to through links. Attaching cryptographic hashes to these links would allow to verify the overall integrity of the set of interconnected graphs. One such example is the <a href="https://www.w3.org/TR/owl2-primer/#a_imports">import mechanism of OWL</a>: the ontology consumer may wish to verify that the imported ontology is the same as the one used by the author of the importing ontology, otherwise the resulting inferences may differ. Another such example are <a href="https://www.w3.org/TR/EARL10-Schema/">EARL test reports</a>: the consumer may wish to ensure that the test description pointed to by the report is the one that was actually used for the test.
        <br>
        Requirement: A standard way of computing and attaching a cryptographic hash to a graph or dataset.
      </dd>
    </dl>
  </section>

</body>

</html>
